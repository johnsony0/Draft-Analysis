{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import Adam\n",
    "from sklearn.svm  import LinearSVC\n",
    "from sklearn.naive_bayes  import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "#loads global variables\n",
    "\n",
    "load_dotenv()\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "all_champions = np.array(['Aatrox', 'Ahri', 'Akali', 'Akshan', 'Alistar', 'Amumu', 'Anivia',\n",
    "                    'Annie', 'Aphelios', 'Ashe', 'AurelionSol', 'Aurora', 'Azir', 'Bard',\n",
    "                    'Belveth', 'Blitzcrank', 'Brand', 'Braum', 'Briar', 'Caitlyn',\n",
    "                    'Camille', 'Cassiopeia', 'Chogath', 'Corki', 'Darius', 'Diana',\n",
    "                    'DrMundo', 'Draven', 'Ekko', 'Elise', 'Evelynn', 'Ezreal',\n",
    "                    'FiddleSticks', 'Fiora', 'Fizz', 'Galio', 'Gangplank', 'Garen',\n",
    "                    'Gnar', 'Gragas', 'Graves', 'Gwen', 'Hecarim', 'Heimerdinger',\n",
    "                    'Hwei', 'Illaoi', 'Irelia', 'Ivern', 'Janna', 'JarvanIV', 'Jax',\n",
    "                    'Jayce', 'Jhin', 'Jinx', 'KSante', 'Kaisa', 'Kalista', 'Karma',\n",
    "                    'Karthus', 'Kassadin', 'Katarina', 'Kayle', 'Kayn', 'Kennen',\n",
    "                    'Khazix', 'Kindred', 'Kled', 'KogMaw', 'Leblanc', 'LeeSin',\n",
    "                    'Leona', 'Lillia', 'Lissandra', 'Lucian', 'Lulu', 'Lux',\n",
    "                    'Malphite', 'Malzahar', 'Maokai', 'MasterYi', 'Milio',\n",
    "                    'MissFortune', 'MonkeyKing', 'Mordekaiser', 'Morgana', 'Naafiri',\n",
    "                    'Nami', 'Nasus', 'Nautilus', 'Neeko', 'Nidalee', 'Nilah',\n",
    "                    'Nocturne', 'Nunu', 'Olaf', 'Orianna', 'Ornn', 'Pantheon', 'Poppy',\n",
    "                    'Pyke', 'Qiyana', 'Quinn', 'Rakan', 'Rammus', 'RekSai', 'Rell',\n",
    "                    'Renata', 'Renekton', 'Rengar', 'Riven', 'Rumble', 'Ryze',\n",
    "                    'Samira', 'Sejuani', 'Senna', 'Seraphine', 'Sett', 'Shaco', 'Shen',\n",
    "                    'Shyvana', 'Singed', 'Sion', 'Sivir', 'Skarner', 'Smolder', 'Sona',\n",
    "                    'Soraka', 'Swain', 'Sylas', 'Syndra', 'TahmKench', 'Taliyah',\n",
    "                    'Talon', 'Taric', 'Teemo', 'Thresh', 'Tristana', 'Trundle',\n",
    "                    'Tryndamere', 'TwistedFate', 'Twitch', 'Udyr', 'Urgot', 'Varus',\n",
    "                    'Vayne', 'Veigar', 'Velkoz', 'Vex', 'Vi', 'Viego', 'Viktor',\n",
    "                    'Vladimir', 'Volibear', 'Warwick', 'Xayah', 'Xerath', 'XinZhao',\n",
    "                    'Yasuo', 'Yone', 'Yorick', 'Yuumi', 'Zac', 'Zed', 'Zeri', 'Ziggs',\n",
    "                    'Zilean', 'Zoe', 'Zyra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(region='NA1',game_mode='ARAM',patch='14.13'):\n",
    "    #gets data collected into a csv\n",
    "\n",
    "    #if using sqlalchemy\n",
    "    # engine_name = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "    conn = psycopg2.connect(\n",
    "        database = os.getenv('DB_NAME'),\n",
    "        host = os.getenv('DB_HOST'),\n",
    "        user = os.getenv('DB_USER'),\n",
    "        password = os.getenv('DB_PASSWORD'),\n",
    "        port = os.getenv('5432')\n",
    "    )\n",
    "\n",
    "    os.makedirs('MatchData', exist_ok=True)\n",
    "    csv_path = f'MatchData/{region}_{game_mode}_{patch}.csv'\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    patch = patch+'%'\n",
    "\n",
    "    query_sql = \"\"\"SELECT * \n",
    "    FROM match_data \n",
    "    WHERE region = %s \n",
    "    AND game_mode = %s \n",
    "    AND patch LIKE %s\"\"\"\n",
    "\n",
    "    query = cursor.mogrify(query_sql,(region, game_mode,patch))\n",
    "    query = query.decode('utf-8')\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(csv_path):\n",
    "            print(\"Csv found\")\n",
    "        else:\n",
    "            with open(csv_path,'w') as f:\n",
    "                cursor.copy_expert(\"COPY ({}) TO STDOUT WITH CSV HEADER\".format(query),f)\n",
    "            print(\"Copy to csv successful\")\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(region, game_mode, elo, version):\n",
    "    # Initiates SQL engine and uses it to get data given user settings and returns it as a dataframe\n",
    "    engine_name = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "    engine = create_engine(engine_name)\n",
    "    \n",
    "    # Initialize query parts\n",
    "    query_sql = \"SELECT * FROM match_data\"\n",
    "    params = []\n",
    "    \n",
    "    # Add conditions dynamically based on inputs\n",
    "    if region != 'ANY':\n",
    "        if params:\n",
    "            query_sql += \" AND\"\n",
    "        else:\n",
    "            query_sql += \" WHERE\"\n",
    "        query_sql += \" region = %s\"\n",
    "        params.append(region)\n",
    "    \n",
    "    if game_mode != 'ANY':\n",
    "        if params:\n",
    "            query_sql += \" AND\"\n",
    "        else:\n",
    "            query_sql += \" WHERE\"\n",
    "        query_sql += \" game_mode = %s\"\n",
    "        params.append(game_mode)\n",
    "    \n",
    "    if elo != 'ANY':\n",
    "        if params:\n",
    "            query_sql += \" AND\"\n",
    "        else:\n",
    "            query_sql += \" WHERE\"\n",
    "        query_sql += \" elo LIKE %s\"\n",
    "        params.append(elo + '%')\n",
    "    \n",
    "    if version != 'ANY':\n",
    "        if params:\n",
    "            query_sql += \" AND\"\n",
    "        else:\n",
    "            query_sql += \" WHERE\"\n",
    "        query_sql += \" version LIKE %s\"\n",
    "        params.append(version + '%')\n",
    "    \n",
    "    df = pd.read_sql_query(query_sql, con=engine, params=tuple(params))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data(df = None):\n",
    "    #any rows that are corrupted, either through an error in data write or read, are dropped\n",
    "    df = df.dropna()\n",
    "    \n",
    "    blue_team = ['blue_one','blue_two','blue_three','blue_four', 'blue_five']\n",
    "    red_team = ['red_one', 'red_two', 'red_three', 'red_four', 'red_five']\n",
    "    \n",
    "    blue_team_encoded = np.zeros((len(df),len(all_champions)))\n",
    "    blue_team_columns = [f\"blue_{champ}\" for champ in all_champions]\n",
    "    red_team_encoded = np.zeros((len(df),len(all_champions)))\n",
    "    red_team_columns = [f\"red_{champ}\" for champ in all_champions]\n",
    "\n",
    "    #encode the blue and red champions using bag of words \n",
    "    for idx,row in df.iterrows():\n",
    "        for col in blue_team:\n",
    "            champ = row[col]\n",
    "            champ_index = np.where(all_champions == champ)[0]\n",
    "            blue_team_encoded[idx][champ_index] = 1\n",
    "\n",
    "        for col in red_team: \n",
    "            champ = row[col]\n",
    "            champ_index = np.where(all_champions == champ)[0]\n",
    "            red_team_encoded[idx][champ_index] = 1\n",
    "\n",
    "    #convert encoded data from array to dataframe and concatenate the blue and red team dataframes\n",
    "    #also drop columns we will not be using\n",
    "    blue_team_encoded = pd.DataFrame(blue_team_encoded,columns=blue_team_columns)\n",
    "    red_team_encoded = pd.DataFrame(red_team_encoded, columns=red_team_columns)\n",
    "\n",
    "    match_ids = df['match_id']\n",
    "\n",
    "    df = df.drop(columns=['id','region','match_id','game_mode','elo','version'])\n",
    "    df = df.drop(columns=blue_team)\n",
    "    df = df.drop(columns=red_team)\n",
    "    df = pd.concat([df,blue_team_encoded,red_team_encoded],axis=1)\n",
    "\n",
    "    return df, match_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DraftAnalysisNN(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size,16,1)\n",
    "        self.conv2 = nn.Conv1d(16,32,1)\n",
    "        self.fc1 = nn.Linear(len(all_champions) * 32,64)\n",
    "        self.fc2 = nn.Linear(64,output_size)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self,input):\n",
    "        logits = self.act(self.conv1(input))\n",
    "        logits = self.act(self.conv2(logits))\n",
    "        logits = logits.view(logits.size(0),-1)\n",
    "        logits = self.fc1(logits)\n",
    "        logits = self.fc2(logits)\n",
    "        \n",
    "        output = torch.softmax(logits,dim=1)\n",
    "        print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.label = np.array(df.iloc[:,0])\n",
    "        self.input =  np.array(df.iloc[:,1:df.shape[1]])\n",
    "        self.data = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        split = int((self.data.shape[1]-1)/2)\n",
    "        blue_team = self.data.iloc[idx,1:split+1]\n",
    "        red_team = self.data.iloc[idx,split+1:self.data.shape[1]]\n",
    "\n",
    "        blue_team_tensor = torch.tensor(blue_team.values, dtype=torch.float32)\n",
    "        red_team_tensor = torch.tensor(red_team.values, dtype=torch.float32)\n",
    "\n",
    "        input = torch.stack((blue_team_tensor,red_team_tensor),dim=0).to(device)\n",
    "\n",
    "        label = self.data.iloc[idx,0]\n",
    "        label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "        return {\n",
    "            'input': input,\n",
    "            'label' : label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,batch_size):\n",
    "    #split the data of the dataset and array (dataset for nn, array for scipy)\n",
    "    train_indices, test_indices = train_test_split(range(len(data)), test_size=0.2, random_state=42, shuffle=True)\n",
    "    train_dataset = Subset(data, train_indices)\n",
    "    test_dataset = Subset(data, test_indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    train_inputs, test_inputs, train_labels, test_labels = train_test_split(data.input, data.label, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    return train_loader,test_loader, train_inputs, test_inputs, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_loader, train_inputs, train_labels, num_epochs, paths):\n",
    "    #training loop\n",
    "    SVC_model = LinearSVC(dual = 'auto')\n",
    "    GNB_model = GaussianNB()\n",
    "    channels = next(iter(train_loader))['input'].shape[1]\n",
    "    NN_model = DraftAnalysisNN(channels,channels).to(device)\n",
    "    \n",
    "    SVC_model.fit(train_inputs,train_labels)\n",
    "    GNB_model.fit(train_inputs,train_labels)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(NN_model.parameters(),lr=1e-5)\n",
    "\n",
    "    print(f'Starting training of {num_epochs} epochs')\n",
    "    NN_model.zero_grad()\n",
    "    NN_model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            input = batch['input']\n",
    "            label = batch['label']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = NN_model(input)\n",
    "            loss = criterion(output,label)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "    #save model\n",
    "    print(f'Saving models')\n",
    "    with open(paths['SVC'], 'wb') as file:\n",
    "        pickle.dump(SVC_model,file)\n",
    "    \n",
    "    with open(paths['GNB'],'wb') as file:\n",
    "        pickle.dump(GNB_model, file)\n",
    "    \n",
    "    torch.save(NN_model.state_dict(),paths['NN'])\n",
    "\n",
    "    return SVC_model, GNB_model, NN_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(SVC_model,GNB_model, NN_model, test_loader, test_inputs, test_labels):\n",
    "    #test models\n",
    "    SVC_predictions = SVC_model.predict(test_inputs)\n",
    "    GNB_predictions = GNB_model.predict(test_inputs)\n",
    "    \n",
    "    print('Testing Models')\n",
    "    NN_model.eval()\n",
    "    NN_predictions = []\n",
    "    labels = []\n",
    "    voting_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input = batch['input']\n",
    "            label = batch['label'].cpu().numpy()\n",
    "\n",
    "            output = NN_model(input)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            #print(output)\n",
    "            NN_pred = prediction.cpu().numpy()\n",
    "\n",
    "            NN_predictions.extend(NN_pred)\n",
    "            labels.extend(label)\n",
    "\n",
    "            sklearn_input = input.flatten().cpu().numpy().reshape((input.shape[0], -1))\n",
    "            SVC_pred = SVC_model.predict(sklearn_input)\n",
    "            GNB_pred = GNB_model.predict(sklearn_input)\n",
    "\n",
    "            #voting implementation\n",
    "            combined_preds = np.stack([NN_pred, SVC_pred, GNB_pred], axis=0)\n",
    "            voting_result = stats.mode(combined_preds)\n",
    "            voting_predictions.extend(voting_result[0])\n",
    "    \n",
    "    SVC_acc = accuracy_score(SVC_predictions, test_labels)\n",
    "    GNB_acc = accuracy_score(GNB_predictions, test_labels)\n",
    "    NN_acc = accuracy_score(NN_predictions,labels)\n",
    "    voting_acc = accuracy_score(voting_predictions, labels)\n",
    "    \n",
    "    print(f'SVC Accuracy: {SVC_acc:.2f}')\n",
    "    print(f'GNB Accuracy: {GNB_acc:.2f}')\n",
    "    print(f'NN Accuracy: {NN_acc:.2f}')\n",
    "    print(f'Voting Accuracy: {voting_acc:.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(SVC_model,GNB_model,NN_model, blue_team, red_team):\n",
    "    #encode the blue_team and red_team inputs\n",
    "    blue_team_encoded = np.zeros(len(all_champions))\n",
    "    red_team_encoded = np.zeros(len(all_champions))\n",
    "\n",
    "    for champ in blue_team:\n",
    "        champ_index = np.where(all_champions == champ)[0]\n",
    "        if champ_index.size == 0:\n",
    "            raise ValueError(f\"Champion '{champ}' not found in list.\")\n",
    "        blue_team_encoded[champ_index] = 1\n",
    "\n",
    "    for champ in red_team:\n",
    "        champ_index = np.where(all_champions == champ)[0]\n",
    "        if champ_index.size == 0:\n",
    "            raise ValueError(f\"Champion '{champ}' not found in list.\")\n",
    "        red_team_encoded[champ_index] = 1\n",
    "\n",
    "    #create sklearn input as a numpy array and pytorch nn input as a tensor\n",
    "    sklearn_input = np.concatenate((blue_team_encoded,red_team_encoded),axis=None).reshape(1,-1)\n",
    "    SVC_pred = SVC_model.predict(sklearn_input)\n",
    "    GNB_pred = GNB_model.predict(sklearn_input)\n",
    "    \n",
    "    nn_input = torch.vstack((torch.tensor(blue_team_encoded),torch.tensor(red_team_encoded))).float().unsqueeze(0).to(device)\n",
    "    output = NN_model(nn_input)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "    NN_pred = prediction.cpu().numpy()\n",
    "    #voting implementation\n",
    "    combined_preds = np.stack([NN_pred, SVC_pred, GNB_pred], axis=0)\n",
    "    voting_result = stats.mode(combined_preds)[0]\n",
    "\n",
    "    #print results\n",
    "    if voting_result == 0:\n",
    "        winner = 'Blue Team'\n",
    "    else:\n",
    "        winner = 'Red Team'\n",
    "    \n",
    "    winner = 'Blue Team' if voting_result == 0 else 'Red Team'\n",
    "    \n",
    "    print(f'Predicted winner is {winner}')\n",
    "    print(f'SVC model predicted {\"Blue Team\" if SVC_pred[0] == 0 else \"Red Team\"}')\n",
    "    print(f'GNB model predicted {\"Blue Team\" if GNB_pred[0] == 0 else \"Red Team\"}')\n",
    "    print(f'NN model predicted {\"Blue Team\" if NN_pred[0] == 0 else \"Red Team\"} with a {output[0][0]*100 if NN_pred[0] == 0 else output[0][1]*100:.2f}% chance')\n",
    "\n",
    "    return sklearn_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_game(encoded_data, threshold, inputs, labels, match_ids):\n",
    "    matching_elements = (inputs == 1) & (encoded_data == 1)\n",
    "    match_counts = np.sum(matching_elements, axis=1)\n",
    "    matching_ids = match_ids[match_counts >= threshold]\n",
    "    matching_labels = labels[match_counts >= threshold]\n",
    "    \n",
    "    if matching_ids.empty:\n",
    "        print(f'Found no games with {threshold} or more similar characters')\n",
    "    else:\n",
    "        for idx, id in enumerate(matching_ids):\n",
    "            region, id_num = id.split('_')\n",
    "            if region == 'NA1':\n",
    "                region = 'NA'\n",
    "            region = region.lower()\n",
    "            winner = 'Blue Team' if matching_labels[idx] else 'Red Team'\n",
    "            print(f'https://www.leagueofgraphs.com/match/{region}/{id_num}, {winner} won')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5072 games with given settings\n",
      "Models exist, loading models\n",
      "Predicted winner is Blue Team\n",
      "SVC model predicted Blue Team\n",
      "GNB model predicted Blue Team\n",
      "NN model predicted Blue Team with a 62.66% chance\n",
      "https://www.leagueofgraphs.com/match/na/5034529931, Red Team won\n"
     ]
    }
   ],
   "source": [
    "def main(blue_team, red_team,region='NA1',game_mode='ARAM',elo='ANY',version='14.13', threshold=5, batch_size=1, num_epochs=10, override=False):\n",
    "    #error testing\n",
    "    if len(blue_team) != 5 or len(red_team) != 5:\n",
    "        raise ValueError(\"Both teams must have exactly 5 champions. \"\n",
    "                         f\"Current sizes - Blue team: {len(blue_team)}, Red team: {len(red_team)}\")\n",
    "    if len(blue_team) != len(set(blue_team)):\n",
    "        raise ValueError(\"Duplicate champions found in blue team\")\n",
    "    if len(red_team) != len(set(red_team)):\n",
    "        raise ValueError(\"Duplicate champions found in red team\")\n",
    "    \n",
    "    #label paths\n",
    "    prefix = f'Models/{region}_{game_mode}_{elo}_{version}_'\n",
    "    paths = {\n",
    "        'SVC' : prefix+'svc_model.pkl',\n",
    "        'GNB' : prefix+'gnb_model.pkl',\n",
    "        'NN' : prefix+'nn_model.pth'\n",
    "    }\n",
    "\n",
    "    #gets data\n",
    "    df = get_data(region,game_mode,elo,version)\n",
    "    df, match_ids = verify_data(df)\n",
    "    data = MatchDataset(df) \n",
    "    print(f'Found {len(match_ids)} games with given settings')\n",
    "\n",
    "    #train/test models if it does not exist. Otherwise load models and predict whether blue or red team will win based on the given champions\n",
    "    if(override or not all(os.path.exists(path) for path in paths.values())):\n",
    "        print('Override is true or model(s) missing, training/testing models')\n",
    "        train_loader,test_loader, train_inputs, test_inputs, train_labels, test_labels = split_data(data,batch_size)\n",
    "        SVC_model, GNB_model, NN_model = train_models(train_loader,train_inputs, train_labels, num_epochs, paths)\n",
    "        test_models(SVC_model,GNB_model, NN_model, test_loader, test_inputs, test_labels)\n",
    "    else:\n",
    "        print('Models exist, loading models')\n",
    "        with open(paths['SVC'],'rb') as file:\n",
    "            SVC_model = pickle.load(file)\n",
    "    \n",
    "        with open(paths['GNB'],'rb') as file:\n",
    "            GNB_model = pickle.load(file)\n",
    "\n",
    "        NN_model = DraftAnalysisNN(2,2).to(device)\n",
    "        NN_model.load_state_dict(torch.load(paths['NN']))\n",
    "\n",
    "    #prediction\n",
    "    encoded_data = predict(SVC_model,GNB_model,NN_model, blue_team, red_team)\n",
    "    #get old games based on threshold value, if threshold = 5, get all games where 5 champions match up to our 5 champions (on same team)\n",
    "    get_similar_game(encoded_data[0],threshold,data.input,data.label, match_ids)\n",
    "    \n",
    "blue_team = ['Vladimir','Udyr','Caitlyn','Velkoz','Jhin']\n",
    "red_team = ['Karthus','Kalista','Irelia','Garen','Janna']\n",
    "main(blue_team, red_team,'NA1','ARAM','ANY','14.13', threshold=4, batch_size=1, num_epochs=10, override=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
