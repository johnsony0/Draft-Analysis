{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import Adam\n",
    "from sklearn.svm  import LinearSVC\n",
    "from sklearn.naive_bayes  import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(region='NA1',game_mode='ARAM',patch='14.13'):\n",
    "    #gets data collected into a csv\n",
    "\n",
    "    #if using sqlalchemy\n",
    "    # engine_name = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "    conn = psycopg2.connect(\n",
    "        database = os.getenv('DB_NAME'),\n",
    "        host = os.getenv('DB_HOST'),\n",
    "        user = os.getenv('DB_USER'),\n",
    "        password = os.getenv('DB_PASSWORD'),\n",
    "        port = os.getenv('5432')\n",
    "    )\n",
    "\n",
    "    os.makedirs('MatchData', exist_ok=True)\n",
    "    csv_path = f'MatchData/{region}_{game_mode}_{patch}.csv'\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    patch = patch+'%'\n",
    "\n",
    "    query_sql = \"\"\"SELECT * \n",
    "    FROM match_data \n",
    "    WHERE region = %s \n",
    "    AND game_mode = %s \n",
    "    AND patch LIKE %s\"\"\"\n",
    "\n",
    "    query = cursor.mogrify(query_sql,(region, game_mode,patch))\n",
    "    query = query.decode('utf-8')\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(csv_path):\n",
    "            print(\"Csv found\")\n",
    "        else:\n",
    "            with open(csv_path,'w') as f:\n",
    "                cursor.copy_expert(\"COPY ({}) TO STDOUT WITH CSV HEADER\".format(query),f)\n",
    "            print(\"Copy to csv successful\")\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(region='NA1',game_mode='ARAM',elo='ANY',version='14.13'):\n",
    "    #features extraction, encoding and data verification\n",
    "    engine_name = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "    engine = create_engine(engine_name)\n",
    "    \n",
    "    version = version+'%'\n",
    "    elo = elo+'%'\n",
    "\n",
    "    if(elo == 'ANY%'):\n",
    "        query_sql = \"\"\"SELECT * \n",
    "        FROM match_data \n",
    "        WHERE region = %s \n",
    "        AND game_mode = %s \n",
    "        AND version LIKE %s\"\"\"\n",
    "        params = (region,game_mode,version)\n",
    "        \n",
    "    else:\n",
    "        query_sql = \"\"\"SELECT * \n",
    "        FROM match_data \n",
    "        WHERE region = %s \n",
    "        AND game_mode = %s \n",
    "        AND elo LIKE %s\n",
    "        AND version LIKE %s\"\"\"\n",
    "        params = (region, game_mode, elo, version)\n",
    "\n",
    "    df = pd.read_sql_query(query_sql,con=engine,params=params)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data(df = None):\n",
    "    df = df.dropna()\n",
    "\n",
    "    all_champions = np.array(['Aatrox', 'Ahri', 'Akali', 'Akshan', 'Alistar', 'Amumu', 'Anivia',\n",
    "                    'Annie', 'Aphelios', 'Ashe', 'AurelionSol', 'Azir', 'Bard',\n",
    "                    'Belveth', 'Blitzcrank', 'Brand', 'Braum', 'Briar', 'Caitlyn',\n",
    "                    'Camille', 'Cassiopeia', 'Chogath', 'Corki', 'Darius', 'Diana',\n",
    "                    'DrMundo', 'Draven', 'Ekko', 'Elise', 'Evelynn', 'Ezreal',\n",
    "                    'FiddleSticks', 'Fiora', 'Fizz', 'Galio', 'Gangplank', 'Garen',\n",
    "                    'Gnar', 'Gragas', 'Graves', 'Gwen', 'Hecarim', 'Heimerdinger',\n",
    "                    'Hwei', 'Illaoi', 'Irelia', 'Ivern', 'Janna', 'JarvanIV', 'Jax',\n",
    "                    'Jayce', 'Jhin', 'Jinx', 'KSante', 'Kaisa', 'Kalista', 'Karma',\n",
    "                    'Karthus', 'Kassadin', 'Katarina', 'Kayle', 'Kayn', 'Kennen',\n",
    "                    'Khazix', 'Kindred', 'Kled', 'KogMaw', 'Leblanc', 'LeeSin',\n",
    "                    'Leona', 'Lillia', 'Lissandra', 'Lucian', 'Lulu', 'Lux',\n",
    "                    'Malphite', 'Malzahar', 'Maokai', 'MasterYi', 'Milio',\n",
    "                    'MissFortune', 'MonkeyKing', 'Mordekaiser', 'Morgana', 'Naafiri',\n",
    "                    'Nami', 'Nasus', 'Nautilus', 'Neeko', 'Nidalee', 'Nilah',\n",
    "                    'Nocturne', 'Nunu', 'Olaf', 'Orianna', 'Ornn', 'Pantheon', 'Poppy',\n",
    "                    'Pyke', 'Qiyana', 'Quinn', 'Rakan', 'Rammus', 'RekSai', 'Rell',\n",
    "                    'Renata', 'Renekton', 'Rengar', 'Riven', 'Rumble', 'Ryze',\n",
    "                    'Samira', 'Sejuani', 'Senna', 'Seraphine', 'Sett', 'Shaco', 'Shen',\n",
    "                    'Shyvana', 'Singed', 'Sion', 'Sivir', 'Skarner', 'Smolder', 'Sona',\n",
    "                    'Soraka', 'Swain', 'Sylas', 'Syndra', 'TahmKench', 'Taliyah',\n",
    "                    'Talon', 'Taric', 'Teemo', 'Thresh', 'Tristana', 'Trundle',\n",
    "                    'Tryndamere', 'TwistedFate', 'Twitch', 'Udyr', 'Urgot', 'Varus',\n",
    "                    'Vayne', 'Veigar', 'Velkoz', 'Vex', 'Vi', 'Viego', 'Viktor',\n",
    "                    'Vladimir', 'Volibear', 'Warwick', 'Xayah', 'Xerath', 'XinZhao',\n",
    "                    'Yasuo', 'Yone', 'Yorick', 'Yuumi', 'Zac', 'Zed', 'Zeri', 'Ziggs',\n",
    "                    'Zilean', 'Zoe', 'Zyra'])\n",
    "    \n",
    "    blue_team = ['blue_one','blue_two','blue_three','blue_four', 'blue_five']\n",
    "    red_team = ['red_one', 'red_two', 'red_three', 'red_four', 'red_five']\n",
    "\n",
    "    blue_team_encoded = np.zeros((len(df),len(all_champions)))\n",
    "    blue_team_columns = [f\"blue_{champ}\" for champ in all_champions]\n",
    "    red_team_encoded = np.zeros((len(df),len(all_champions)))\n",
    "    red_team_columns = [f\"red_{champ}\" for champ in all_champions]\n",
    "\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        for col in blue_team:\n",
    "            champ = row[col]\n",
    "            champ_index = np.where(all_champions == champ)[0]\n",
    "            blue_team_encoded[idx][champ_index] = 1\n",
    "\n",
    "        for col in red_team:\n",
    "            champ = row[col]\n",
    "            champ_index = np.where(all_champions == champ)[0]\n",
    "            red_team_encoded[idx][champ_index] = 1\n",
    "\n",
    "    blue_team_encoded = pd.DataFrame(blue_team_encoded,columns=blue_team_columns)\n",
    "    red_team_encoded = pd.DataFrame(red_team_encoded, columns=red_team_columns)\n",
    "\n",
    "    df = df.drop(columns=['id','match_id','region','game_mode','elo','version'])\n",
    "    df = df.drop(columns=blue_team)\n",
    "    df = df.drop(columns=red_team)\n",
    "    df = pd.concat([df,blue_team_encoded,red_team_encoded],axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DraftAnalysisNN(nn.Module):\n",
    "    def __init__(self,input_size,output_size,kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,1)\n",
    "        self.conv2 = nn.Conv2d(16,32,1)\n",
    "        self.fc1 = nn.Linear(167 * 2 * 32,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self,input):\n",
    "        logits = self.act(self.conv1(input))\n",
    "        logits = self.act(self.conv2(logits))\n",
    "        logits = logits.view(1,-1)\n",
    "        logits = self.fc1(logits)\n",
    "        logits = self.fc2(logits)\n",
    "        \n",
    "        output = torch.softmax(logits,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        blue_team = np.array(df.iloc[:,1:168])\n",
    "        red_team = np.array(df.iloc[:,168:355])\n",
    "        self.label = df.iloc[:,0]\n",
    "        self.input = np.concatenate((blue_team,red_team),axis=1)\n",
    "        self.data = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        blue_team = self.data.iloc[idx,1:168]\n",
    "        red_team = self.data.iloc[idx,168:335]\n",
    "\n",
    "        blue_team_tensor = torch.tensor(blue_team.values, dtype=torch.float32).to(device)\n",
    "        red_team_tensor = torch.tensor(red_team.values, dtype=torch.float32).to(device)\n",
    "\n",
    "        input = torch.stack((blue_team_tensor,red_team_tensor),dim=0)\n",
    "\n",
    "        label = self.data.iloc[idx,0]\n",
    "        label = torch.tensor(label, dtype=torch.float32).to(device)\n",
    "\n",
    "        return {\n",
    "            'input': input,\n",
    "            'label' : label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,batch_size):\n",
    "    train_indices, test_indices = train_test_split(range(len(data)), test_size=0.2, random_state=42, shuffle=True)\n",
    "    train_dataset = Subset(data, train_indices)\n",
    "    test_dataset = Subset(data, test_indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    train_inputs, test_inputs, train_labels, test_labels = train_test_split(data.input, data.label, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    return train_loader,test_loader, train_inputs, test_inputs, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_loader, train_inputs, train_labels, num_epochs):\n",
    "    SVC_model = LinearSVC(dual = 'auto')\n",
    "    GNB_model = GaussianNB()\n",
    "    NN_model = DraftAnalysisNN(1,2,1)\n",
    "    \n",
    "    print(train_inputs[0].shape)\n",
    "    SVC_model.fit(train_inputs,train_labels)\n",
    "    GNB_model.fit(train_inputs,train_labels)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(NN_model.parameters(),lr=1e-4)\n",
    "\n",
    "    '''\n",
    "    print(f'Starting training of {num_epochs} epochs')\n",
    "    NN_model.zero_grad()\n",
    "    NN_model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            input = batch['input']\n",
    "            label = batch['label'].long()\n",
    "            \n",
    "            print(input)\n",
    "            print(input.shape)\n",
    "            output = NN_model(input)\n",
    "            loss = criterion(output,label)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "        '''\n",
    "    return SVC_model, GNB_model, NN_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(SVC_model,GNB_model, NN_model, test_loader, test_inputs, test_labels):\n",
    "    SVC_predictions = SVC_model.predict(test_inputs)\n",
    "    GNB_predictions = GNB_model.predict(test_inputs)\n",
    "\n",
    "    '''\n",
    "    print('Testing NN')\n",
    "    NN_model.eval()\n",
    "    NN_predictions = []\n",
    "    labels = []\n",
    "    voting_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input = batch['input']\n",
    "            label = batch['label'].cpu().numpy()\n",
    "\n",
    "            output = NN_model(input)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            NN_pred = prediction.cpu().numpy()\n",
    "\n",
    "            NN_predictions.extend(NN_pred)\n",
    "            labels.extend(label)\n",
    "\n",
    "            sklearn_input = input.flatten().cpu().numpy().reshape((input.shape[0], -1))\n",
    "            SVC_pred = SVC_model.predict(sklearn_input)\n",
    "            GNB_pred = GNB_model.predict(sklearn_input)\n",
    "\n",
    "            combined_preds = np.stack([NN_pred, SVC_pred, GNB_pred], axis=1)\n",
    "            voting_result = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=combined_preds)\n",
    "            voting_predictions.extend(voting_result)\n",
    "    '''\n",
    "    SVC_acc = accuracy_score(SVC_predictions, test_labels)\n",
    "    GNB_acc = accuracy_score(GNB_predictions, test_labels)\n",
    "    #NN_acc = accuracy_score(NN_predictions,labels)\n",
    "    #voting_acc = accuracy_score(voting_predictions, labels)\n",
    "    \n",
    "    print(f'SVC Accuracy: {SVC_acc:.2f}')\n",
    "    print(f'GNB Accuracy: {GNB_acc:.2f}')\n",
    "    #print(f'NN Accuracy: {NN_acc:.2f}')\n",
    "    #print(f'Voting Accuracy: {voting_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "SVC Accuracy: 0.54\n",
      "GNB Accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "def main(region,game_mode,elo,version, batch_size,num_epochs):\n",
    "    #get_csv(region=region,game_mode=game_mode,patch=patch)\n",
    "    #data extraction\n",
    "    df = get_data(region,game_mode,elo,version)\n",
    "    #data encoding + validation\n",
    "    df = verify_data(df) \n",
    "    data = MatchDataset(df) \n",
    "    #build model + training \n",
    "    train_loader,test_loader, train_inputs, test_inputs, train_labels, test_labels = split_data(data,batch_size)\n",
    "    SVC_model, GNB_model, NN_model = train_models(train_loader,train_inputs, train_labels, num_epochs)\n",
    "    test_models(SVC_model,GNB_model, NN_model, test_loader, test_inputs, test_labels)\n",
    "\n",
    "main('NA1','ARAM','ANY','14.13', batch_size = 1, num_epochs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
